{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] Fused region process have not been installed. Please refer to get_started.md for installation.\n",
      "[Warning] Fused region process have not been installed. Please refer to get_started.md for installation.\n",
      "Setting tau to 1.0\n",
      "Setting tau to 1.0\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "from modules import attmil,clam,dsmil,transmil,rrt\n",
    "import torch\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "sys.path.append('./DTFD/')\n",
    "from Model.Attention import Attention_with_Classifier\n",
    "from Model.Attention import Attention_Gated as Attention\n",
    "from Model.network import Classifier_1fc, DimReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attmil.DAttention(2,dropout=0.25,act='relu',test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = attmil.AttentionGated(dropout=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clam.CLAM_SB(n_classes=2,dropout=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = clam.CLAM_MB(n_classes=2,dropout=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dsmil.MILNet(2,0.25,'relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transmil.TransMIL(n_classes=2,dropout=0.25,act='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rrt.RRT(pos='none',peg_k=7,attn='rrt',pool='attn',n_layers=2,epeg=True,conv_k=15,ffn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTFD\n",
    "def get_cam_1d(classifier, features):\n",
    "    tweight = list(classifier.parameters())[-2]\n",
    "    cam_maps = torch.einsum('bgf,cf->bcg', [features, tweight])\n",
    "    return cam_maps\n",
    "class DTFD(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.classifier = Classifier_1fc(512, 2, 0.25)\n",
    "        self.attention = Attention(512)\n",
    "        self.dimReduction = DimReduction(1024, 512, dropout=0.25)\n",
    "        self.UClassifier = Attention_with_Classifier(L=512, num_cls=2, droprate=0.25)\n",
    "        self.group = 5\n",
    "        self.distill = 'MaxMinS'\n",
    "    def forward(self, x):\n",
    "        print(x.size())\n",
    "        #x = x[0]\n",
    "        feat_index = list(range(x.shape[0]))\n",
    "        index_chunk_list = np.array_split(np.array(feat_index), self.group)\n",
    "        index_chunk_list = [sst.tolist() for sst in index_chunk_list]\n",
    "        slide_pseudo_feat = []\n",
    "        slide_sub_preds = []\n",
    "        for tindex in index_chunk_list:\n",
    "            \n",
    "            subFeat_tensor = torch.index_select(x, dim=0, index=torch.LongTensor(tindex))\n",
    "            tmidFeat = self.dimReduction(subFeat_tensor)\n",
    "            tAA = self.attention(tmidFeat).squeeze(0)\n",
    "\n",
    "            tattFeats = torch.einsum('ns,n->ns', tmidFeat, tAA)  ### n x fs\n",
    "            tattFeat_tensor = torch.sum(tattFeats, dim=0).unsqueeze(0)  ## 1 x fs\n",
    "            tPredict = self.classifier(tattFeat_tensor)  ### 1 x 2\n",
    "            slide_sub_preds.append(tPredict)\n",
    "\n",
    "            patch_pred_logits = get_cam_1d(self.classifier, tattFeats.unsqueeze(0)).squeeze(0)  ###  cls x n\n",
    "            patch_pred_logits = torch.transpose(patch_pred_logits, 0, 1)  ## n x cls\n",
    "            patch_pred_softmax = torch.softmax(patch_pred_logits, dim=1)  ## n x cls\n",
    "\n",
    "            _, sort_idx = torch.sort(patch_pred_softmax[:,-1], descending=True)\n",
    "            topk_idx_max = sort_idx[:1].long()\n",
    "            topk_idx_min = sort_idx[-1:].long()\n",
    "            topk_idx = torch.cat([topk_idx_max, topk_idx_min], dim=0)\n",
    "\n",
    "            MaxMin_inst_feat = tmidFeat.index_select(dim=0, index=topk_idx)   ##########################\n",
    "            max_inst_feat = tmidFeat.index_select(dim=0, index=topk_idx_max)\n",
    "            af_inst_feat = tattFeat_tensor\n",
    "\n",
    "            if self.distill == 'MaxMinS':\n",
    "                slide_pseudo_feat.append(MaxMin_inst_feat)\n",
    "            elif self.distill == 'MaxS':\n",
    "                slide_pseudo_feat.append(max_inst_feat)\n",
    "            elif self.distill == 'AFS':\n",
    "                slide_pseudo_feat.append(af_inst_feat)\n",
    "\n",
    "        slide_pseudo_feat = torch.cat(slide_pseudo_feat, dim=0)  ### \n",
    "\n",
    "        gSlidePred = self.UClassifier(slide_pseudo_feat)\n",
    "        return gSlidePred\n",
    "model = DTFD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "506\n",
      "FLOPs = 10.061387008G\n",
      "Params = 1.181954M\n"
     ]
    }
   ],
   "source": [
    "flops, params = profile(model, inputs=torch.rand(1, 9000, 1024))\n",
    "print('FLOPs = ' + str(flops/1000**3) + 'G')\n",
    "print('Params = ' + str(params/1000**2) + 'M')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
